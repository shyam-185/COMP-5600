{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shyam Patel\n",
    "sjp0059\n",
    "Assignment 5\n",
    "\n",
    "Problem 2:\n",
    "\n",
    "1: \n",
    "The state space (S) is each cell in the 25x25 grid, each cell is a possible state.\n",
    "The action space (A) are the possible moves the agent can make (Up, Down, Left, Right) NO diagonal moves.\n",
    "The transition probabilities (P) All movement is deterministic so there is no randomness, which means that the probabilities will be 1 for moving in valid directions. For invalid movements the probaility is 1 that the agent will stay in place and 0 otherwise.\n",
    "The reward function (R) each move will give the agent -1.\n",
    "The discount factor (Î³) will be at 0 temporarily, because from what I understand we just need to get the shortest path to goal, there are no rewards on the way and each step gives us -1 so we need to take the least amount of steps. We may move to a small discount factor of 0.1 or 0.2 if 0 does not work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of policy iterations: 24\n",
      "\n",
      "Optimal policy:\n",
      " G < B ^ < B v v v < v v v B v v v v v v v B > > v\n",
      " ^ ^ < B B v v v v B v v v B v v v v v v v < B B v\n",
      " ^ ^ ^ < < < < < < < < < < < < < < < < < < B B v v\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ < < < <\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ < ^ ^ ^ B ^ B B ^ ^ ^ ^ ^ ^ B\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ B ^ ^ < ^ < B ^ ^ ^ ^ B ^ <\n",
      " ^ ^ ^ B ^ B ^ ^ B B ^ < ^ ^ ^ ^ ^ < ^ B ^ ^ B ^ B\n",
      " ^ ^ ^ < ^ < B B B v ^ ^ ^ ^ ^ B ^ ^ B v B B v B ^\n",
      " B ^ ^ ^ ^ ^ < < < < ^ ^ B ^ B ^ B ^ < < < < < < B\n",
      " B B ^ B B ^ ^ ^ ^ ^ B ^ < B v B > ^ ^ ^ ^ ^ ^ ^ <\n",
      " B > ^ < < ^ ^ ^ ^ ^ < ^ ^ < < B ^ ^ ^ ^ ^ B ^ ^ ^\n",
      " v B ^ ^ ^ B B ^ ^ B ^ ^ B ^ ^ B ^ ^ ^ ^ B v ^ ^ ^\n",
      " > > ^ ^ ^ < < B B > ^ B B ^ B v B B ^ ^ < < B ^ B\n",
      " ^ B ^ ^ ^ ^ ^ B v B ^ < < ^ < < < < ^ ^ B ^ B ^ <\n",
      " ^ B B ^ ^ ^ ^ < < < ^ ^ ^ ^ ^ B ^ B ^ B v ^ < ^ ^\n",
      " B > > ^ ^ B ^ ^ B ^ B B ^ ^ ^ B ^ B ^ < < B ^ ^ ^\n",
      " > ^ ^ B B > ^ B B ^ < < B ^ ^ < ^ < ^ ^ ^ < ^ ^ B\n",
      " ^ B ^ < B ^ ^ < < ^ ^ ^ < ^ B ^ ^ ^ ^ ^ B ^ ^ ^ B\n",
      " ^ < B ^ < ^ B ^ ^ ^ ^ ^ ^ ^ < ^ ^ B B B > ^ ^ ^ <\n",
      " ^ ^ > ^ ^ ^ < B ^ B B ^ ^ ^ ^ B ^ < < < B ^ B ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ < ^ B v ^ ^ B ^ < ^ B ^ ^ < ^ < B ^\n",
      " ^ B ^ ^ B ^ B ^ ^ < < B ^ B B ^ ^ < ^ ^ B ^ ^ < ^\n",
      " ^ B B B B ^ < ^ B B ^ < ^ B v ^ ^ ^ ^ B v ^ ^ ^ B\n",
      " ^ < < B > ^ ^ B v > ^ ^ ^ < < ^ ^ ^ ^ < < ^ ^ ^ <\n",
      " ^ ^ ^ < ^ ^ ^ < < ^ ^ B ^ B ^ ^ ^ B ^ ^ ^ B ^ ^ B\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_size = 25\n",
    "blocked = -1\n",
    "empty = 0\n",
    "goal = (0, 0)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "\n",
    "discount_factor = 0.95\n",
    "threshold = 0.001\n",
    "\n",
    "# Initial Random Policy\n",
    "np.random.seed()\n",
    "grid = np.zeros((grid_size, grid_size), dtype=int)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if np.random.rand() < 0.2 and (i, j) != goal:\n",
    "            grid[i, j] = blocked\n",
    "grid[goal] = empty\n",
    "\n",
    "policy = {}\n",
    "value_function = {}\n",
    "\n",
    "for x in range(grid_size):\n",
    "    for y in range(grid_size):\n",
    "        if grid[x, y] != blocked:\n",
    "            policy[(x, y)] = np.random.choice(actions)\n",
    "            value_function[(x, y)] = 0\n",
    "            \n",
    "# Get state\n",
    "def get_next_state(state, action):\n",
    "    if state == goal:\n",
    "        return state\n",
    "    x, y = state\n",
    "    dx, dy = action_effects[action]\n",
    "    nx, ny = x + dx, y + dy\n",
    "    if 0 <= nx < grid_size and 0 <= ny < grid_size and grid[nx, ny] != blocked:\n",
    "        return (nx, ny)\n",
    "    return state\n",
    "\n",
    "# Policy Evaluation to calculate the value function for the policy.\n",
    "def policy_evaluation(policy, value_function):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in value_function:\n",
    "            if state == goal:\n",
    "                continue\n",
    "            old_value = value_function[state]\n",
    "            action = policy[state]\n",
    "            next_state = get_next_state(state, action)\n",
    "            new_value = -1 + discount_factor * value_function[next_state]\n",
    "            value_function[state] = new_value\n",
    "            delta = max(delta, abs(old_value - new_value))\n",
    "        if delta < threshold:\n",
    "            break\n",
    "\n",
    "# Policy Improvement to update the policy based on the value function.\n",
    "def policy_improvement(policy, value_function):\n",
    "    policy_stable = True\n",
    "    for state in value_function:\n",
    "        if state == goal:\n",
    "            continue\n",
    "        old_policy = policy[state]\n",
    "        best_action = None\n",
    "        best_action_value = float('-inf')\n",
    "        for action in actions:\n",
    "            next_state = get_next_state(state, action)\n",
    "            action_value = -1 + discount_factor * value_function[next_state]\n",
    "            if action_value > best_action_value:\n",
    "                best_action_value = action_value\n",
    "                best_action = action\n",
    "        policy[state] = best_action\n",
    "        if best_action != old_policy:\n",
    "            policy_stable = False\n",
    "    return policy_stable\n",
    "\n",
    "# Number of Iterations\n",
    "iteration_count = 0\n",
    "while True:\n",
    "    policy_evaluation(policy, value_function)\n",
    "    if policy_improvement(policy, value_function):\n",
    "        break\n",
    "    iteration_count += 1\n",
    "\n",
    "print(f\"Number of policy iterations: {iteration_count}\")\n",
    "\n",
    "# Visualization\n",
    "def visualize(policy):\n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            state = (x, y)\n",
    "            if state == goal:\n",
    "                print(' G', end='')\n",
    "            elif grid[x, y] == blocked:\n",
    "                print(' B', end='')\n",
    "            elif state in policy:\n",
    "                arrow = ''\n",
    "                if policy[state] == 'up':\n",
    "                    arrow = '^'\n",
    "                elif policy[state] == 'down':\n",
    "                    arrow = 'v'\n",
    "                elif policy[state] == 'left':\n",
    "                    arrow = '<'\n",
    "                elif policy[state] == 'right':\n",
    "                    arrow = '>'\n",
    "                print(f' {arrow}', end='')\n",
    "        print()\n",
    "\n",
    "print(\"\\nOptimal policy:\")\n",
    "visualize(policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report:\n",
    "a. I used numpy arrays because it is easy to implement compared to other libraries. NumPy arrays provide well-suited data structure choices for MDP. It is efficient and supports vectorized operations which work well with the computational requirements of MDP solution algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy:\n",
      " G < < < < < < < < < < < < < < < < < < < < < < < <\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ B B B B B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ B B B B B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ B B B B B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ B B B B B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ B B B B B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ < < < < < B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ B ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ < ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ B B B B B ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ < < < < < ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      " ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^\n",
      "Median number of steps to reach the goal state (Random): 30.0\n",
      "Median number of steps to reach the goal state (Intermediate): 23.0\n",
      "Median number of steps to reach the goal state (Optimal): 16.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_size = 25\n",
    "blocked = -1\n",
    "empty = 0\n",
    "goal = (0, 0)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
    "\n",
    "discount_factor = 0.95\n",
    "threshold = 0.001\n",
    "\n",
    "# Predefined grid with no blocked cells\n",
    "grid = np.zeros((grid_size, grid_size), dtype=int)\n",
    "grid[10:15, 10] = blocked  # Adding vertical obstacle\n",
    "grid[20, 15:20] = blocked  # Adding horizontal obstacle\n",
    "grid[5:10, 5:10] = blocked  # Adding square obstacle\n",
    "\n",
    "grid[goal] = empty\n",
    "\n",
    "policy = {}\n",
    "value_function = {}\n",
    "\n",
    "for x in range(grid_size):\n",
    "    for y in range(grid_size):\n",
    "        if grid[x, y] != blocked:\n",
    "            policy[(x, y)] = np.random.choice(actions)  # Initialize policy randomly\n",
    "            value_function[(x, y)] = 0  # Initialize value function to zeros\n",
    "\n",
    "# Number of Iterations\n",
    "iteration_count = 0\n",
    "while True:\n",
    "    policy_evaluation(policy, value_function)\n",
    "    if policy_improvement(policy, value_function):\n",
    "        break\n",
    "    iteration_count += 1\n",
    "\n",
    "\n",
    "# Visualization\n",
    "def visualize(policy):\n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            state = (x, y)\n",
    "            if state == goal:\n",
    "                print(' G', end='')\n",
    "            elif grid[x, y] == blocked:\n",
    "                print(' B', end='')\n",
    "            elif state in policy:\n",
    "                arrow = ''\n",
    "                if policy[state] == 'up':\n",
    "                    arrow = '^'\n",
    "                elif policy[state] == 'down':\n",
    "                    arrow = 'v'\n",
    "                elif policy[state] == 'left':\n",
    "                    arrow = '<'\n",
    "                elif policy[state] == 'right':\n",
    "                    arrow = '>'\n",
    "                print(f' {arrow}', end='')\n",
    "        print()\n",
    "\n",
    "print(\"Optimal policy:\")\n",
    "visualize(policy)\n",
    "\n",
    "# Function to simulate policy execution\n",
    "def simulate_policy(policy, grid, start_state):\n",
    "    current_state = start_state\n",
    "    steps = 0\n",
    "    while current_state != goal:\n",
    "        action = policy[current_state]\n",
    "        next_state = get_next_state(current_state, action)\n",
    "        if next_state == current_state:  # Agent hits a blocked cell\n",
    "            return float('inf')\n",
    "        current_state = next_state\n",
    "        steps += 1\n",
    "    return steps\n",
    "\n",
    "# Function to compute performance of a policy\n",
    "def compute_policy_performance(policy, grid, num_trials=3):\n",
    "    start_states = [np.random.randint(0, grid_size, size=2) for _ in range(num_trials)]\n",
    "    steps_to_goal = []\n",
    "    for start_state in start_states:\n",
    "        steps = simulate_policy(policy, grid, tuple(start_state))\n",
    "        steps_to_goal.append(steps)\n",
    "    median_steps = np.median(steps_to_goal)\n",
    "    return median_steps\n",
    "\n",
    "# Run the simulation\n",
    "performance = compute_policy_performance(policy, grid)\n",
    "print(\"Median number of steps to reach the goal state (Random):\", performance)\n",
    "\n",
    "grid[goal] = empty\n",
    "\n",
    "# Function to initialize policy and value function\n",
    "def initialize_policy_and_value(grid):\n",
    "    policy = {}\n",
    "    value_function = {}\n",
    "    for x in range(grid_size):\n",
    "        for y in range(grid_size):\n",
    "            if grid[x, y] != blocked:\n",
    "                policy[(x, y)] = np.random.choice(actions)  # Initialize policy randomly\n",
    "                value_function[(x, y)] = 0  # Initialize value function to zeros\n",
    "    return policy, value_function\n",
    "\n",
    "\n",
    "# Function to run policy iteration and return the policy and value function\n",
    "def run_policy_iteration(grid):\n",
    "    policy, value_function = initialize_policy_and_value(grid)\n",
    "    iteration_count = 0\n",
    "    while True:\n",
    "        policy_evaluation(policy, value_function)\n",
    "        if policy_improvement(policy, value_function):\n",
    "            break\n",
    "        iteration_count += 1\n",
    "    return policy\n",
    "\n",
    "# Generate random start states for all tests\n",
    "start_states = [np.random.randint(0, grid_size, size=2) for _ in range(3)]\n",
    "\n",
    "# Run policy iteration for intermediate policy\n",
    "intermediate_policy = run_policy_iteration(grid)\n",
    "intermediate_performance = compute_policy_performance(intermediate_policy, grid)\n",
    "print(\"Median number of steps to reach the goal state (Intermediate):\", intermediate_performance)\n",
    "\n",
    "# Run policy iteration for optimal policy\n",
    "optimal_policy= run_policy_iteration(grid)\n",
    "optimal_performance = compute_policy_performance(optimal_policy, grid)\n",
    "print(\"Median number of steps to reach the goal state (Optimal):\", optimal_performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the median tests, this took me a really long time because I was using my code in the previous section that randomly generates a grid. The code kept running into an infinite loop. What I had no realized after a long time was that the agent kept moving between the same 2 states, resulting in an infinite loop. This can happen in instances where there are block cells that do not allow the agent to move anywhere but 1 place, thus it creates an infinite loop. So, what I did was create a predefined maze where there is 0 chance of gettning stuck in an infinite loop. \n",
    "\n",
    "The agent starts at a randomly generated seed, and that position is used for all 3 tests (Random, Optimal, Intermediate). One thing to note is that if the agent spawns at a blocked cell the program will output an error, so just simply rerun it until the agent starts at a non-blocked cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
